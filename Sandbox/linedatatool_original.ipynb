{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748abb71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Select * from [dbo].[NeuMoDx_Green_Normalized_Master]where ([Result Code] = 'FRS1') and [Start Time]>'2022-03-15' and [Start Time] < '2022-05-23'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SuS\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Select * from [dbo].[NeuMoDx_Yellow_Normalized_Master]where ([Result Code] = 'FRS1') and [Start Time]>'2022-03-15' and [Start Time] < '2022-05-23'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SuS\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Select * from [dbo].[NeuMoDx_Orange_Normalized_Master]where ([Result Code] = 'FRS1') and [Start Time]>'2022-03-15' and [Start Time] < '2022-05-23'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SuS\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Select * from [dbo].[NeuMoDx_Far_Red_Normalized_Master]where ([Result Code] = 'FRS1') and [Start Time]>'2022-03-15' and [Start Time] < '2022-05-23'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SuS\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Select * from [dbo].[NeuMoDx_Red_Normalized_Master]where ([Result Code] = 'FRS1') and [Start Time]>'2022-03-15' and [Start Time] < '2022-05-23'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SuS\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#####################READ ME#####################\n",
    "'''\n",
    "Name:\n",
    "V&V Line data maker\n",
    "\n",
    "Introduction:\n",
    "Using V&V database to create line data for V&V studies - FRS1\n",
    "\n",
    "Requirements:\n",
    "Python 3.8\n",
    "SQL Server Management Studio V18.10\n",
    "\n",
    "Script Author:\n",
    "Szu-Chi(Caine) Su (Szuchi.Su@qiagen.com)\n",
    "\n",
    "Acknowledge:\n",
    "This database was created and currently being maintained by Aaron Riley.\n",
    "'''\n",
    "#####################READ ME END#####################\n",
    "\n",
    "#####################Universial functions#####################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import array\n",
    "import pyodbc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from IPython.display import HTML\n",
    "config = dict(server='ABR-RIPLEYA-D1',\n",
    "    port=      1433,\n",
    "    database=  'neumodxVVLabDB',\n",
    "    )\n",
    "\n",
    "conn_str = ('SERVER={server};' +\n",
    "            'Database={database};' +\n",
    "            'TRUSTED_CONNECTION=yes')\n",
    "\n",
    "conn = pyodbc.connect(\n",
    "    r'DRIVER={SQL Server};' +\n",
    "    conn_str.format(**config)\n",
    "    )\n",
    "\n",
    "def getSQLData_Channel(color, dtype, assay, start_time_filter, end_date_filter):\n",
    "    query = (\"Select * from [dbo].[NeuMoDx_\"+color+\"_\"+dtype+\"_Master]\"+\n",
    "             \"where ([Result Code] = '\"+assay+\"') and [Start Time]>'\"+start_time_filter+\"'\"+\n",
    "             \" and [Start Time] < '\"+end_date_filter+\"'\")\n",
    "    print(\"Query:\", query)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    return df\n",
    "             \n",
    "def getSQLData_COC(assay, start_time_filter, end_date_filter):\n",
    "    query = (\"Select * from [dbo].[NeuMoDx_System_Master_Table]\"+\n",
    "             \" where ([Assay Name] = '\"+assay+\"') and [Start Date Time]>'\"+start_time_filter+\"'\"+\n",
    "             \" and [Start Date Time] < '\"+end_date_filter+\"'\")\n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(\"Query:\", query)\n",
    "    return df\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "def xlfile():\n",
    "    xlfile = []\n",
    "    for fn in os.listdir():\n",
    "        if fn[-4:] == \"xlsx\" and fn[:13] == \"RawDataExport\":\n",
    "            xlfile.append(fn)\n",
    "    return xlfile\n",
    "def mylist():\n",
    "    mylist = []\n",
    "    for file in xlfile():\n",
    "        df = pd.read_excel(file, 1) \n",
    "        mylist.append(df['Test Guid'].tolist())\n",
    "    return flatten(mylist)\n",
    "\n",
    "mylist = mylist()\n",
    "#####################Universial functions end#####################\n",
    "\n",
    "#######Green channel#######\n",
    "#Filtering data points by date\n",
    "dfg = getSQLData_Channel('Green', 'Normalized', 'FRS1', '2022-03-15','2022-05-23')\n",
    "filter1 = dfg['Test Guid'].isin(mylist)\n",
    "dfg = dfg[filter1]\n",
    "dfg1 = dfg.filter(items=['Sample ID', 'Replicate Result','Target Result','Flags','Ct','Max Peak Height','EPR','End Point Fluorescence','N500 Serial Number', 'Start Time',\"Test Guid\",])\n",
    "dfg1 = dfg1.rename(columns={'Sample ID': 'sampleid','N500 Serial Number': 'system_number','Ct' : 'Ctg'})\n",
    "#Filtering data points by system\n",
    "# dfg1 = dfg1[(dfg1.system_number == \"N000012\")|(dfg1.system_number == \"N000013\")|(dfg1.system_number == \"N000004\")|(dfg1.system_number == \"96000004\")|(dfg1.system_number == \"96000005\")|(dfg1.system_number == \"12000073\")|(dfg1.system_number == \"12000075\")|(dfg1.system_number == \"12000017\")]\n",
    "# #Filtering out the unwanted data points(ex:controls)\n",
    "# dfg1 = dfg1[(dfg1.sampleid != \"FRSPC11607Y221104\")&(dfg1.sampleid != \"FRSNC11607X221104\")&(dfg1.sampleid != \"FRSNC\")&(dfg1.sampleid != \"FRSPC\")]\n",
    "\n",
    "#######Yellow channel#######\n",
    "#Filtering data points by date\n",
    "dfy = getSQLData_Channel('Yellow', 'Normalized', 'FRS1', '2022-03-15','2022-05-23')\n",
    "filter1 = dfy['Test Guid'].isin(mylist)\n",
    "dfy = dfy[filter1]\n",
    "dfy1 = dfy.filter(items=['Sample ID', 'Target Result','Flags','Ct','Max Peak Height','EPR','End Point Fluorescence','N500 Serial Number', 'Start Time',\"Test Guid\"])\n",
    "dfy1 = dfy1.rename(columns={'Sample ID': 'sampleid','N500 Serial Number': 'system_number','Ct' : 'Cty'})\n",
    "#Filtering data points by system\n",
    "# dfy1 = dfy1[(dfy1.system_number == \"N000012\")|(dfy1.system_number == \"N000013\")|(dfy1.system_number == \"N000004\")|(dfy1.system_number == \"96000004\")|(dfy1.system_number == \"96000005\")|(dfy1.system_number == \"12000073\")|(dfy1.system_number == \"12000075\")|(dfy1.system_number == \"12000017\")]\n",
    "# #Filtering out the unwanted data points(ex:controls)\n",
    "# dfy1 = dfy1[(dfy1.sampleid != \"FRSPC11607Y221104\")&(dfy1.sampleid != \"FRSNC11607X221104\")&(dfy1.sampleid != \"FRSNC\")&(dfy1.sampleid != \"FRSPC\")]\n",
    "\n",
    "#######Orange channel#######\n",
    "#Filtering data points by date\n",
    "dfo = getSQLData_Channel('Orange', 'Normalized', 'FRS1', '2022-03-15','2022-05-23')\n",
    "filter1 = dfo['Test Guid'].isin(mylist)\n",
    "dfo = dfo[filter1]\n",
    "dfo1 = dfo.copy()\n",
    "dfo1 = dfo.filter(items=['Sample ID', 'Target Result','Flags','Ct','Max Peak Height','EPR','End Point Fluorescence','N500 Serial Number', 'Start Time',\"Test Guid\"])\n",
    "dfo1 = dfo1.rename(columns={'Sample ID': 'sampleid','N500 Serial Number': 'system_number','Ct' : 'Cto'})\n",
    "#Filtering data points by system\n",
    "# dfo1 = dfo1[(dfo1.system_number == \"N000012\")|(dfo1.system_number == \"N000013\")|(dfo1.system_number == \"N000004\")|(dfo1.system_number == \"96000004\")|(dfo1.system_number == \"96000005\")|(dfo1.system_number == \"12000073\")|(dfo1.system_number == \"12000075\")|(dfo1.system_number == \"12000017\")]\n",
    "# #Filtering out the unwanted data points(ex:controls)\n",
    "# dfo1 = dfo1[(dfo1.sampleid != \"FRSPC11607Y221104\")&(dfo1.sampleid != \"FRSNC11607X221104\")&(dfo1.sampleid != \"FRSNC\")&(dfo1.sampleid != \"FRSPC\")]\n",
    "\n",
    "#######Far red channel#######\n",
    "#Filtering data points by date\n",
    "dffr = getSQLData_Channel('Far_Red', 'Normalized', 'FRS1', '2022-03-15','2022-05-23')\n",
    "filter1 = dffr['Test Guid'].isin(mylist)\n",
    "dffr = dffr[filter1]\n",
    "dffr1 = dffr.filter(items=['Sample ID', 'Target Result','Flags','Ct','Max Peak Height','EPR','End Point Fluorescence','N500 Serial Number', 'Start Time',\"Test Guid\"])\n",
    "dffr1 = dffr1.rename(columns={'Sample ID': 'sampleid','N500 Serial Number': 'system_number','Ct' : 'Ctfr'})\n",
    "#Filtering data points by system\n",
    "# dffr1 = dffr1[(dffr1.system_number == \"N000012\")|(dffr1.system_number == \"N000013\")|(dffr1.system_number == \"N000004\")|(dffr1.system_number == \"96000004\")|(dffr1.system_number == \"96000005\")|(dffr1.system_number == \"12000073\")|(dffr1.system_number == \"12000075\")|(dffr1.system_number == \"12000017\")]\n",
    "# #Filtering out the unwanted data points(ex:controls)\n",
    "# dffr1 = dffr1[(dffr1.sampleid != \"FRSPC11607Y221104\")&(dffr1.sampleid != \"FRSNC11607X221104\")&(dffr1.sampleid != \"FRSNC\")]\n",
    "\n",
    "#######Red channel#######\n",
    "#Filtering data points by date\n",
    "dfr = getSQLData_Channel('Red', 'Normalized', 'FRS1', '2022-03-15','2022-05-23')\n",
    "filter1 = dfr['Test Guid'].isin(mylist)\n",
    "dfr = dfr[filter1]\n",
    "dfr1 = dfr.filter(items=['Sample ID', 'Target Result','Flags','Ct','Max Peak Height','EPR','End Point Fluorescence','N500 Serial Number', 'Start Time',\"Test Guid\"])\n",
    "dfr1 = dfr1.rename(columns={'Sample ID': 'sampleid','N500 Serial Number': 'system_number','Ct' : 'Ctr'})\n",
    "#Filtering data points by system\n",
    "# dfr1 = dfr1[(dfr1.system_number == \"N000012\")|(dfr1.system_number == \"N000013\")|(dfr1.system_number == \"N000004\")|(dfr1.system_number == \"96000004\")|(dfr1.system_number == \"96000005\")|(dfr1.system_number == \"12000073\")|(dfr1.system_number == \"12000075\")|(dfr1.system_number == \"12000017\")]\n",
    "# #Filtering out the unwanted data points(ex:controls)\n",
    "# dfr1 = dfr1[(dfr1.sampleid != \"FRSPC11607Y221104\")&(dfr1.sampleid != \"FRSNC11607X221104\")&(dfr1.sampleid != \"FRSNC\")&(dfr1.sampleid != \"FRSPC\")]\n",
    "\n",
    "#Merging all dataframe from different colors\n",
    "dfm1 = pd.merge(dfg1, dfy1, on = 'Test Guid')\n",
    "dfm2 = pd.merge(dfo1, dffr1, on = 'Test Guid')\n",
    "dfm3 = pd.merge(dfm1, dfm2, on = 'Test Guid')\n",
    "dfline = pd.merge(dfm3, dfr1, on = 'Test Guid')\n",
    "\n",
    "#Clean up the column names\n",
    "dfline = dfline.filter(items=['Start Time','sampleid_x_x', 'Replicate Result','Target Result_x_x','Flags_x_x','Ctg','Max Peak Height_x_x','EPR_x_x','End Point Fluorescence_x_x', 'Target Result_y_x','Flags_y_x','Cty','Max Peak Height_y_x','EPR_y_x','End Point Fluorescence_y_x', 'Target Result_x_y','Flags_x_y','Cto','Max Peak Height_x_y','EPR_x_y','End Point Fluorescence_x_y','Target Result_y_y','Flags_y_y','Ctfr','Max Peak Height_y_y','EPR_y_y','End Point Fluorescence_y_y', 'Target Result','Flags','Ctr','Max Peak Height','EPR','End Point Fluorescence','system_number'])\n",
    "dfline = dfline.rename(columns= {'sampleid_x_x':'Sample_ID', 'Target Result_x_x':'Target Result_Green','Flags_x_x':'Flags','Ctg':'Ct','Max Peak Height_x_x':'MPH','EPR_x_x':'EPR','End Point Fluorescence_x_x':'EP', 'Target Result_y_x':'Target Result_Yellow','Flags_y_x':'Flags','Cty':'Ct','Max Peak Height_y_x':'MPH','EPR_y_x':'EPR','End Point Fluorescence_y_x':'EP', 'Target Result_x_y':'Target Result_Orange','Flags_x_y':'Flags','Cto':'Ct','Max Peak Height_x_y':'MPH','EPR_x_y':'EPR','End Point Fluorescence_x_y':'EP','Target Result_y_y':'Target Result_Far_Red','Flags_y_y':'Flags','Ctfr':'Ct','Max Peak Height_y_y':'MPH','EPR_y_y':'EPR','End Point Fluorescence_y_y': 'EP','Target Result':'Target Result_Red','Max Peak Height':'MPH','End Point Fluorescence':'EP','Ctr':'Ct'})\n",
    "dfline = dfline.replace(\"TargetAmplified\", \"AMP\")\n",
    "dfline = dfline.replace(\"TargetNotAmplified\", \"NoAMP\")\n",
    "dfline = dfline.replace(\"NoResult\", \"NR\")\n",
    "dfline = dfline.replace(\"TargetUnresolved\", \"UNR\")\n",
    "dfline = dfline.replace(\"Unresolved\", \"UNR\")\n",
    "dfline = dfline.replace(\"TargetIndeterminate\", \"IND\")\n",
    "dfline = dfline.replace(\"Indeterminate\", \"IND\")\n",
    "dfline = dfline.replace(\"1020 (Informational, Peak Not Detected)\", \"1020\")\n",
    "dfline = dfline.replace(\"1020 (Informational, Peak Not Detected), 1039 (Informational, Overall EPR Threshold Check Failed)\", \"1020, 1039\")\n",
    "\n",
    "#File output, change line_test.csv to your desired file name\n",
    "dfline.drop_duplicates(inplace = True)\n",
    "dfline = dfline.round({'Yellow Target Ct':2,\n",
    "            'Yellow Target EP':0,\n",
    "            'Yellow Target Max Peak Height':0,\n",
    "            'Yellow Target EPR':2,\n",
    "            'Green Target Ct':2,\n",
    "            'Green Target EP':0,\n",
    "            'Green Target Max Peak Height':0,\n",
    "            'Green Target EPR':2,\n",
    "            'Orange Target Ct':2,\n",
    "            'Orange Target EP':0,\n",
    "            'Green Target Max Peak Height':0,\n",
    "            'Green Target EPR':2,\n",
    "            'Far Red Target Ct':2,\n",
    "            'Far Red Target EP':0,\n",
    "            'Far Red Target Max Peak Height':0,\n",
    "            'Far Red Target EPR':2,\n",
    "            'Red Target Ct':2,\n",
    "            'Red Target EP':0,\n",
    "            'Red Target Max Peak Height':0,\n",
    "            'Red Target EPR':2})\n",
    "\n",
    "flag_dict = {'1000 \\(Error, User Aborted Test\\)':'1000',\n",
    "            '1002 \\(Error, Insufficient Raw Readings\\)':'1002',\n",
    "            '1004 \\(Informational, Fixed Baseline Used\\)':'1004',\n",
    "            '1005 \\(Informational, End Point Fluorescence Met Failed\\)':'1005',\n",
    "            '1006 \\(Informational, Peak Location Failed\\)':'1006',\n",
    "            '1012 \\(Error, Outlier Removal Failed\\)':'1012',\n",
    "            '1013 \\(Informational, Fill Check Failed\\)':'1013',\n",
    "            '1020 \\(Informational, Peak Not Detected\\)':'1020',\n",
    "            '1024 \\(Error, Starting Fluorescence Exceeded\\)':'1024',\n",
    "            '1025 \\(Informational, Below EPR Threshold\\)':'1025',\n",
    "            '1029 \\(Informational, Repeat Testing Recommended\\)':'1029',\n",
    "            '1031 \\(Error, EPR Threshold Not Met\\)':'1031',\n",
    "            '1034 \\(Informational, Inhibition Detected\\)':'1034',\n",
    "            '1035 \\(Informational, Ct Below Fixed Baseline Start\\)':'1035',\n",
    "            '1037 \\(Informational, A very early amplification was potentially detected in the samples. It may be beneficial to dilute the starting sample 1:1000 and repeat.\\)':'1037',\n",
    "            '1038 \\(Informational, Single Point Normalization Applied\\)':'1038',\n",
    "            '1039 \\(Informational, Overall EPR Threshold Check Failed\\)':'1039',\n",
    "            '1040 \\(Informational, Derivative Data Filter Implemented\\)':'1040',\n",
    "            '2010 \\(Error, System Error PCR Only\\)':'2010',\n",
    "            '2035 \\(Error, User Shutdown\\)':'2035',\n",
    "            '2300 \\(Error, Module Error\\)':'2300',\n",
    "            '2307 \\(Error, Cartridge not detected in XPCR module\\)':'2307',\n",
    "            '2316 \\(Error, Lane Error\\)':'2316',\n",
    "            '2610 \\(Informational, Delays in sample processing\\)':'2610',\n",
    "            '2618 \\(Error, Maximum LhpA Reschedules Attempted\\)':'2618',\n",
    "            '2619 \\(Error, Maximum LhpB Reschedules Attempted\\)':'2619',\n",
    "            '2620 \\(Error, Maximum LhpC Reschedules Attempted\\)':'2620',\n",
    "            '5075 \\(Error, XPCR Module Failure\\)':'5057',\n",
    "            '7007 \\(Error, Instrument movement error.\\)':'7007',\n",
    "            '7160 \\(Warning, Sample aspiration failure\\)':'7160',\n",
    "            '7161 \\(Warning, No sample detected\\)':'7161',\n",
    "            '7162 \\(Warning, Quantity not sufficient\\)':'7162',\n",
    "            '7163 \\(Error, Clot detected\\)':'7163',\n",
    "            '7340 \\(Warning, Sample dispense failure\\)':'7340',\n",
    "            '7620 \\(Warning, Cartridge dispense failure\\)':'7620'}\n",
    "    \n",
    "for key in flag_dict:\n",
    "    dfline = dfline.replace(regex=[key], value=flag_dict[key])\n",
    "\n",
    "dfline.to_csv('Line_Com.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acbad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72c7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
